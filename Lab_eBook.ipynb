{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab eBook \n",
    "\n",
    "Research Journal to keep a record of daily achievements and performance through out the research project. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 1 - Sept 5th - Sept 9th \n",
    "\n",
    "__Sept 5__ \n",
    "\n",
    "- Intro talk with Anna\n",
    "- Chrunchomics documentation and tutorial, resulting in configuration of personal account \n",
    "- Entered the omics server through the head node and explored: ssh -X 14108550@omics-h0.science.uva.nl \n",
    "- IMP3 documentation: https://imp3.readthedocs.io/en/latest/ \n",
    "\n",
    "__Sept 6__ \n",
    "\n",
    "\n",
    "- Checklist:\n",
    "    - Installed miniconda on omics server personal account \n",
    "    - Watched Lecture 1 of Metagenomics course\n",
    "        - lecture 1: https://video.uva.nl/media/Metagenomics+101+-+1%7C+overview+-+private+version/0_pvgszh0o \n",
    "            - definition of metagenomics\n",
    "            - brief history of metagenomics\n",
    "            - glimpse into raw data\n",
    "            - typical steps in an analysis\n",
    "        - Metagenomics 101: https://staff.fnwi.uva.nl/a.u.s.heintzbuschart/metagenomics_00.html \n",
    "    - Completed Tutorial 1 of Metagenomics course \n",
    "        - Set up: didn't initialize miniconda since it had already been used before and saved data in personal directory.\n",
    "        - First test-run of IMP3 in Crunchomics - dry run to test configuration_\n",
    "            - /zfs/omics/projects/metatools/TOOLS/IMP3/runIMP3 -d my.config.yaml\n",
    "            - desired output: This was a dry-run (flag -n). The order of jobs does __not__ reflect the order of execution.\n",
    "        - Submitted the first test run to the compute nodes of crunchomics server in node 3 with omics-cn003.\n",
    "            - first check the available nodes: sinfo -o \"%n %e %m %a %c %C\" \n",
    "        - IMP3 inputs: data and configuration file \n",
    "            - my.config.yaml configuration file is the only required argument and specifies the input data. \n",
    "            - code used to commit the real run to the compute node, which includes more arguments than the previous dry run: \n",
    "                - /zfs/omics/projects/metatools/TOOLS/IMP3/runIMP3 -c -r -n TESTRUN -b omics-cn002 my.config.yaml \n",
    "    - Read Snakemake documentation \n",
    "        - Tutorial: https://snakemake.readthedocs.io/en/stable/tutorial/tutorial.html#\n",
    "        - Academic Article: https://f1000research.com/articles/10-33/v2 \n",
    "        - website: https://snakemake.github.io/  \n",
    "    - Created github repository and started lab eBook\n",
    "\n",
    "\n",
    "__Sept 7__ \n",
    "\n",
    "- Checklist:\n",
    "    - Metagenomics Lecture 2: Raw Data Preprocessing and QC \n",
    "        - .fastq files tools resource: http://barcwiki.wi.mit.edu/wiki/SOPs/qc_shortReads \n",
    "        - brief look into different sequencing techniques\n",
    "        - what can go wrong when sequencing\n",
    "        - sequencing quality scores\n",
    "        - quality reports\n",
    "        - adapter, spike-in, and host genome removal (the host microbiome needs to be removed because you are interested in the other genomes in the sample)\n",
    "    - Metagenomics Tutorial 2: Run Preprocessing on Toy Data\n",
    "        - Finding Novaseq file in reads data - the GCEF.test.r1.fq and GCEF.test.r2.fq are novaseq\n",
    "        - code used to modify the configuration file __prep.1.config.yaml__ to use datasets COSMIC_vag.test.r1.fq and COSMIC_vag.test.r2.fq instead of the default test data, by manually changing inside the configuration file by using nano command: \n",
    "            - raws:\n",
    "                Metagenomics: \"/zfs/omics/projects/metatools/SANDBOX/Metagenomics101/EXAMPLE_DATA/READS/COSMIC_vag.test.r1.fq /zfs/omics/projects/metatools/SANDBOX/Metagenomics101/EXAMPLE_DATA/READS/COSMIC_vag.test.r2.fq\"\n",
    "        - made a copy of prep.1.config.yaml to be able to run both configuration at the same time and have their own output directories. \n",
    "        - code used to modify the configuration file __prep.2.config.yaml__ to use datasets GCEF.test.r1.fq and GCEF.test.r2.fq instead of the default test data, by manually changing inside the configuration file by using __nano__ command: \n",
    "            - raws:\n",
    "                Metagenomics: \"/zfs/omics/projects/metatools/SANDBOX/Metagenomics101/EXAMPLE_DATA/READS/GCEF.test.r1.fq /zfs/omics/projects/metatools/SANDBOX/Metagenomics101/EXAMPLE_DATA/READS/GCEF.test.r2.fq\"\n",
    "            - also changed the output directory to __IMP3_test_preprocessing_novaseq__, and nextseq argument to __true__. \n",
    "        - performed the dry run and obtained desired output:\n",
    "            - /zfs/omics/projects/metatools/TOOLS/IMP3/runIMP3 -d prep.1.config.yaml \n",
    "            - /zfs/omics/projects/metatools/TOOLS/IMP3/runIMP3 -d prep.2.config.yaml \n",
    "        - both runs were commited to node 2:\n",
    "            - /zfs/omics/projects/metatools/TOOLS/IMP3/runIMP3 -c -r -n TESTRUN -b omics-cn002 prep.1.config.yaml\n",
    "            - /zfs/omics/projects/metatools/TOOLS/IMP3/runIMP3 -c -r -n TESTRUN -b omics-cn002 prep.2.config.yaml\n",
    "        - The dataset COSMIC_vag.test.r1|2.fq comes from a sample that contains still host genomics DNA\n",
    "            - prep.1.config.yaml was copied to prep.3.config.yaml and a new configuration was specified with the same test data, a new output directory __IMP3_test_preprocessing_hg38filtering__, and the filtering was specified to hg38 to filter out the human genome \n",
    "            - dry run was performed and successful \n",
    "            - run was submitted to node 4: \n",
    "                - /zfs/omics/projects/metatools/TOOLS/IMP3/runIMP3 -c -r -n TESTRUN -b omics-cn004 prep.3.config.yaml\n",
    "    - read IMP3 documentation \n",
    "    - read Research project rubric \n",
    "    - read antiSMASH paper \n",
    "\n",
    "\n",
    "__Sept 8__ \n",
    "\n",
    "- checklist:\n",
    "    - Metagenomics 101 Lecture 3: Mapping Metagenomics read data to reference genomes \n",
    "        - why mapping\n",
    "        - bioinformatics for sequence alignment\n",
    "        - look at output (alignments) created by read mappers\n",
    "        - what can be done with the output  \n",
    "    - Tutorial 3:\n",
    "        - not IMP3 pipeline but the commands on their own  \n",
    "        - environments IMP3 provides on crunchomics to preprocess data with BWA-MEM - Burrows Wheeler algoritm (transform) - Maximum Exact Matches \n",
    "        1. Running BWA-MEM indexing \n",
    "            - build index of references (don't do on the frontend if using a real genomes, okay now bc test data)\n",
    "            - cp /zfs/omics/projects/metatools/SANDBOX/Metagenomics101/EXAMPLE_DATA/REFERENCES/mg.assembly.merged.fa . (copied to my personal directory)\n",
    "            - conda activate /zfs/omics/projects/metatools/TOOLS/IMP3/conda/76e02d85877600c41ac84cb7bc27a189\n",
    "            - bwa index mg.assembly.merged.fa \n",
    "        2. Running mapping with BWA-MEM \n",
    "            - bwa mem -v 1 -t 1  mg.assembly.merged.fa /zfs/omics/projects/metatools/SANDBOX/Metagenomics101/EXAMPLE_DATA/READS/test.mg.1.fastq /zfs/omics/projects/metatools/SANDBOX/Metagenomics101/EXAMPLE_DATA/READS/test.mg.2.fastq >> test_alignment.sam\n",
    "            - you can run other data, for examble: COMSIC_vag.test.r1|r2.fq with the hg38.fa - first need to find hg38.fa file to copy to personal directory and index it \n",
    "        3. Take a look at the alignment file \n",
    "            - output of BWA-MEM is an alignment file in .sam format. Alignment information is at the end of the file \n",
    "            - tail test_alignment.sam \n",
    "        4. Converting the alignment file to a binary (compressed) format \n",
    "            - convert to .bam format because the .sam is very big and you don't want to store on own disk \n",
    "            - samtools view --threads 1 -bS test_alignment.sam > test_alignment.bam\n",
    "            - can also be donde in one step with the mapping: \n",
    "                - bwa mem -v 1 -t 1  /zfs/omics/projects/metatools/DB/filtering/hg38.fa /zfs/omics/projects/metatools/SANDBOX/Metagenomics101/EXAMPLE_DATA/READS/COSMIC_vag.test.r1.fq /zfs/omics/projects/metatools/SANDBOX/Metagenomics101/EXAMPLE_DATA/READS/COSMIC_vag.test.r2.fq |\\\n",
    "                    samtools view --threads 1 -bS - > COSMIC_alignment.bam\n",
    "            - can look at a .bam file with samtools view: samtools view test_alignment.bam | less\n",
    "        5. Filtering the .bam file \n",
    "            - also using samtools view, use -f 4 to find only reads which don't map to reference \n",
    "            - samtools view -f 4 test_alignment.bam | less \n",
    "            - this shows many of the values to be 0 \n",
    "            - -F 4 give reads which map (careful, there's also a second read)\n",
    "            - more samtools flags: https://broadinstitute.github.io/picard/explain-flags.html\n",
    "            - samtools manual: http://www.htslib.org/doc/samtools-view.html \n",
    "        6. More complex filtering \n",
    "            - used in IMP3 to remove reads mapping to a host genome \n",
    "            - samtools merge --threads 1 -u - \\\n",
    "                <(samtools view --threads 1 -u  -f 4 -F 264 COSMIC_alignment.bam) \\\n",
    "                <(samtools view --threads 1 -u -f 8 -F 260 COSMIC_alignment.bam) \\\n",
    "                <(samtools view --threads 1 -u -f 12 -F 256 COSMIC_alignment.bam)  | \\\n",
    "            samtools view --threads 1 -bF 0x800 - | \\\n",
    "            samtools sort --threads 1 -m 3G -n - | \\\n",
    "            bamToFastq -i stdin -fq COSMIC_filtered.r1.fq -fq2 COSMIC_filtered.r2.fq\n",
    "\n",
    "            conda deactivate \n",
    "            (all COSMIC_alignment.bam files are the test_alignment.bam files when doing the alignment of COSMIC files to the hg38.fa reference genome - didn't actually perform that alignment) (figure out what the code on top is doing by looking at the resources of flags and samtools documentation)\n",
    "    - read papers \n",
    "    - read Research project rubric \n",
    "    - understand IMP3 documentation very well \n",
    "    - review crunchomics documentation (SLURMs)\n",
    "\n",
    "\n",
    "__Sept 9__ \n",
    "\n",
    "- 11 am meeting with Anna \n",
    "    - where data will go \n",
    "    - expectations\n",
    "    - research project rubric \n",
    "    - questions:\n",
    "        - trial presentation 1-2 weeks before final presentation - november \n",
    "        - final presentation 2 weeks after final draft submitted - schedule myself \n",
    "        - is it confidential? NOT \n",
    "        - examiner final presentation availability - once a month on a friday \n",
    "        - DeskForm mentioned that days off should be accounted for and arranged for another time. \n",
    "        - miniconda33 mistake - uninstalled - reinstall \n",
    "    - meeting notes:\n",
    "        - project data is not nextseq \n",
    "        - will start setting up assemblies next week\n",
    "        - I need to send CV and information for the job contract \n",
    "        - fix miniconda3 installation \n",
    "        - GO/NOGO meeting with plant research group during second week of October \n",
    "        - there are 54 samples in the project data set \n",
    "\n",
    "- checklist:\n",
    "    - watch metagenomics lecture 4: Taxonomy\n",
    "        - different approaches toprofile microbiomes in terms of the present taxa, using metagenomics read data \n",
    "        - introduction into the aims and particular environments \n",
    "        - a touch of taxonomy \n",
    "        - brief reminder of mapping of metagenomis reads \n",
    "        - look at non-alignment based profilers \n",
    "        - how taxonomic profilers are benchmarked \n",
    "        - what we can do with this output \n",
    "    - Tutorial 4: Run mOTUs2, MetaPhlan2, and kraken2\n",
    "        - Use the taxonomic profiles in the IMP3 pipeline as well as MetaPhlan3 (IMP3 in crunchomics and metaphlan3 conda environment)\n",
    "        - use cd /zfs/omics/projects/metatools/SANDBOX/Metagenomics101/EXAMPLE_DATA/READS files or the preprocessed files from the last sessions\n",
    "        - Running taxonomic profiling within __IMP3__ \n",
    "            - start IMP3 with an appropriate configuration file since data is preprocessed. Set config file to not do preprocessing, and set input as already preprocessed files\n",
    "            > cp /zfs/omics/projects/metatools/SANDBOX/Metagenomics101/04_taxonomy1/tax.config.yaml my.tax.config.yaml \n",
    "            - change (using nano) to input own data. Can also change the kraken databse to one of the names in /zfs/omics/projects/metatools/DB with the word kraken in it (UHGP_kraken2 for human gut genomes, minikraken2 for an even smaller database; kraken_pfp8 is the current kraken db in configuration). Output directory: test_tax_output\n",
    "                >raws:\n",
    "                >>Metagenomics: /home/14108550/personal/IMP3_test_preprocessing/Preprocessing/mg.r1.trimmed.phiX174_filtered.fq\n",
    "                >>/home/14108550/personal/IMP3_test_preprocessing/Preprocessing/mg.r2.trimmed.phiX174_filtered.fq\n",
    "                >>/home/14108550/personal/IMP3_test_preprocessing/Preprocessing/mg.se.trimmed.phiX174_filtered.fq\n",
    "            - first did dry run, which was successful \n",
    "            > /zfs/omics/projects/metatools/TOOLS/IMP3/runIMP3 -d my.tax.config.yaml\n",
    "            - then commited run to node 3 \n",
    "            > /zfs/omics/projects/metatools/TOOLS/IMP3/runIMP3 -c -r -n TESTTAX -b omics-cn003 my.tax.config.yaml\n",
    "            - inside the output directory will be another directory Analysis/taxonomy which holds the outputs from mOTUs2 and kraken2 \n",
    "        - Taxonomic profiling using __MetaPhlan3__\n",
    "            - works very well on human samples and has a strain-level module. \n",
    "            - activate the conda environment: \n",
    "            > conda activate /zfs/omics/projects/metatools/TOOLS/miniconda3/envs/metaphlan-3.0\n",
    "            - to get help and documentation on the profiler:\n",
    "            > metaphlan -h \n",
    "            - example to run on filtered test reads:\n",
    "            > metaphlan /zfs/omics/projects/metatools/SANDBOX/Metagenomics101/EXAMPLE_DATA/FILTERED_READS/mg.r1.trimmed.phiX174_filtered.fq,/zfs/omics/projects/metatools/SANDBOX/Metagenomics101/EXAMPLE_DATA/FILTERED_READS/mg.r2.trimmed.phiX174_filtered.fq,/zfs/omics/projects/metatools/SANDBOX/Metagenomics101/EXAMPLE_DATA/FILTERED_READS/mg.se.trimmed.phiX174_filtered.fq --bowtie2out filtered_test.bowtie2.bz2 --input_type fastq -o profile.filtered_test.txt  --bowtie2db /zfs/omics/projects/metatools/DB/metaphlan --unknown_estimation\n",
    "            - can also run on own filtered reads instead. to run on real data set, put the commands to activate (and deactivate) conda and the metaphlan command into a script that you submit to the cluster. You can then also choose to set the --nproc argument (--nproc 8 to parallelize the profiling) \n",
    "            - code for runnin the non-filtered reads \n",
    "            > metaphlan /zfs/omics/projects/metatools/SANDBOX/Metagenomics101/EXAMPLE_DATA/READS/test.mg.r1.fastq,/zfs/omics/projects/metatools/SANDBOX/Metagenomics101/EXAMPLE_DATA/READS/test.mg.r2.fastq --bowtie2out unfiltered_test.bowtie2.bz2 --input_type fastq -o profile.unfiltered_test.txt  --bowtie2db /zfs/omics/projects/metatools/DB/metaphlan --unknown_estimation\n",
    "            - To merge the profiles from several samples (or in this case, the same sample, but represented by filtered or unfiltered reads), you can use metaphlan’s merge script\n",
    "            > merge_metaphlan_tables.py profile*filtered_test.txt > merged_abundance.test.tsv\n",
    "            - to compare the results:\n",
    "            > less merged_abundance.test.tsv\n",
    "            > conda deactivate\n",
    "            \n",
    "    - read papers \n",
    "    - finish metagenomics tutorials to start working on project data next week \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 2 Sept 12 - Sept 16 \n",
    "\n",
    "__Sept 12__\n",
    "\n",
    "- checklist:\n",
    "    - Metagenomics 101 Lecture 5: Assembly\n",
    "        - lecture video: https://video.uva.nl/media/Metagenomics+101+-+5%7C+Assembly+-+private+version/0_p8gnde38 \n",
    "        - why assemble\n",
    "        - alternatives to assembly\n",
    "        - how does assembly work\n",
    "        - how to inspect assemblies\n",
    "        - what to assemble \n",
    "        - stopped at min 17 \n",
    "    - Tutorial 5: Run assemblies in IMP3 \n",
    "        - assemble with IMP3 and look at diagnostic plots \n",
    "\n",
    "__Sept 13__\n",
    "\n",
    "- 11 am meeting with Anna \n",
    "    - srun -w omics-cn002 --pty bash (to access a node's bash) \n",
    "    - ls -lh (to see the details of contents in a directory)\n",
    "    - /zfs/omics/projects/metatools/TOOLS/IMP3/config/config.imp.yaml (IMP3 default config file with some explanations)\n",
    "    - 2 controls and 3 different gene transforms for tomato plant lines, also 1 negative control (no plant, just soil) \n",
    "    - make a logs and configs directories and commit runs through logs \n",
    "- Might start setting up assemblies \n",
    "- Bora's data: /zfs/omics/projects/phb/bora/test/all_samples \n",
    "- made personal directories in the /scratch of each node \n",
    "- commit to nodes:\n",
    "    > run dry runs of config files in a computation node by entering the node and running from there in the logs directory: /zfs/omics/projects/metatools/TOOLS/IMP3/runIMP3 -d ../configs/T_712_2.assembly.config.yaml (done for each of the config files)\n",
    "- sinfo -o \"%n %e %m %a %c %C\"\n",
    "- squeue -u 14108550\n",
    "- -b works only with -c and bind the run to a node \n",
    "- /zfs/omics/projects/metatools/TOOLS/IMP3/runIMP3 -h (for more information on the input flags)\n",
    "    > /zfs/omics/projects/metatools/TOOLS/IMP3/runIMP3 -c -r -n T_712_2 -b omics-cn003 ../configs/T_712_2.assembly.config.yaml\n",
    "    > /zfs/omics/projects/metatools/TOOLS/IMP3/runIMP3 -c -r -n T_712_4 -b omics-cn004 ../configs/T_712_4.assembly.config.yaml\n",
    "    > /zfs/omics/projects/metatools/TOOLS/IMP3/runIMP3 -c -r -n T_712_6 -b omics-cn005 ../configs/T_712_6.assembly.config.yaml\n",
    "\n",
    "__Sept 14__\n",
    "\n",
    "- Checklist: \n",
    "    - Metagenomics Lecture 6: Metagenome Annotation (of assembled contigs)\n",
    "        - finding bacterial genes\n",
    "            - protein coding genes\n",
    "            - rRNAs\n",
    "        - annotating genes with functions\n",
    "            - why not just align?\n",
    "            - HMMs and HMMER\n",
    "    - Tutorial 6: \n",
    "\n",
    "__Sept 15__\n",
    "\n",
    "- Observed that Tuesday's runs were done so submitted 5 more into the computation nodes, one for each node, after successfully running dry runs for each one. Submitted the first 5 samples for T_722 plant line, in ascending order. \n",
    "- 1:30 pm meeting with Anna \n",
    "    - What to do with assembled and analyzed (annotated) reads -> start running antiSMASH \n",
    "    - is there a 9 am meeting tomorrow? NO\n",
    "    - github repository \n",
    "    - how long will the GO/NOGO presentation be? 20 mins\n",
    "    - agriculture and crop management or another aim? -> fundamental microorganisms and plant interactions\n",
    "    - wait until all the assemblies are done to analyse the functional abundance differences. or how to build functional profiles from the assembly and annotation? \n",
    "    - notes:\n",
    "        - **copy the report.html into my personal and then download to my computer to observe the running times and how it all went, if it got stuck somewhere or something**\n",
    "        - **also check the status directory in each sample to make sure everything is fine**\n",
    "        - **delete Preprocessing directory after runs are finished after status is checked, to free up space**\n",
    "        - annotation_CDS_RNA_hmms.gff\tannotation.filt.gff, both of this in Analysis/annotation are the final annotation files that can either be inputs into antiSMASH, check which one is accepted\n",
    "        - the assembly output file to use for antiSMASH: mg.assembly.merged.fa \n",
    "        - can use either a fundamental scope of research to look at interaction between microbiomes and plants to make it more application focused and talk about crop management and agriculture \n",
    "        - always run antiSMASH for a sample in the same computation node as IMP3 assembly was done, to be able to access the input files from scratch \n",
    "        - all of the .counts. files in Analysis/annotation is the analysis of mapped count reads to the assembled genome for each of the specific databases used. \n",
    "        - **figure out antiSMASH on my own**\n",
    "\n",
    "\n",
    "\n",
    "__Sept 16__\n",
    "\n",
    "- antismash\n",
    "    > conda activate /zfs/omics/projects/metatools/TOOLS/miniconda3/envs/antismash6/ \n",
    "    > antismash -h (to learn how to run it with flags and everything)\n",
    "    > conda deactivate once done using it \n",
    "- to move files from remote server to local do ifconfig -a to get inet IP adress, first one, and then do the following code from a local terminal to pull the file to local. the . is to indicate the directory if you're already in that directory \n",
    "> scp 14108550@146.50.10.60:/home/14108550/personal/Samples_Outputs/T_712_2_report.html .\n",
    "- do scancel +number of run to cancel a run \n",
    "- rm -r Preprocessing (to delete directory)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 3 Sept 19 - Sept 23 \n",
    "\n",
    "__Sept 19__\n",
    "\n",
    "- Metagenomics 101 Lecture 7 \n",
    "  - approaches to make (semi-)quantitative profiles of the molecular functions found in metagenomes and how they can be compared across samples\n",
    "  - review on mapping and annotation\n",
    "  - gene abundance measures - functional profile calculation:\n",
    "    - reads per gene/reads per function\n",
    "    - reads per kilobase\n",
    "    - copies per million\n",
    "    - average depth of coverage\n",
    "- antiSMASH documentation \n",
    "    - conda activate /zfs/omics/projects/metatools/TOOLS/miniconda3/envs/antismash6\n",
    "    - antismash -h (do something with it)\n",
    "    - conda deactivate \n",
    "    - antismash --help-showall \n",
    "    - --genefinding-gff3\n",
    "- checked and recorded the results of assembly and annotation runs. Runs that seemed to run well had Preprocessing directory removed. So far only one run had a problem where the .se. was empty \n",
    "- scp -r 14108550@146.50.10.60:/home/14108550/personal/Samples_Outputs .\n",
    "\n",
    "\n",
    "__Sept 20__\n",
    "\n",
    "- tried an antiSMASH run, not successful: \n",
    "> antismash /scratch/slopez/T_722_5_output/Assembly/mg.assembly.merged.fa --genefinding-gff3 /scratch/slopez/T_722_5_output/Analysis/annotation/annotation_CDS_RNA_hmms.gff -c 8 \n",
    "\n",
    "  --genefinding-gff3 GFF3_FILE\n",
    "                        Specify GFF3 file to extract features from.\n",
    "\n",
    "__Sept 21__\n",
    "\n",
    "- 9:30 meeting with Anna \n",
    "  - faulty assembly run\n",
    "  - how to run antiSMASH in computation nodes -> do not run in the nodes, copy the data from scratch to my personal directory since it's not a lot (2 input files) and run in head node \n",
    "  - use the annotation.filt.gff instead of the _CDS_RNA_hmms.gff because the .filf. is produced earlier in the annotation process so it is more raw data and also doesn't have to wait for the other to go into antiSMASH (i'm guess for the IMP3 pipeline)\n",
    "  - using a subset of the long contigs, since the chances of antiSMASH of finding something useful comes from longer contigs\n",
    "    - use BBmap script: __reformat.sh__ on the assembly output file \n",
    "    - min-length=10,000 bp to make the long.toy.assembly dataset, fastaminlen=1\n",
    "    - then use this assembly file to filter the annotation file, to only include those genes. \n",
    "  - could also do a small version of the long contigs (increasing the filtering length?)\n",
    "  - Do a random contigs subset so see what antiSMASH finds\n",
    "- filtered mg.assembly.merged.fa to mg.assembly.merged.length_filtered.fa with 10,000 bp threshold \n",
    "> reformat.sh in=mg.assembly.merged.fa out=mg.assembly.merged.length_filtered.fa minlength=10000\n",
    "> java -ea -Xms300m -cp /zfs/omics/software/packages/bbmap/current/ jgi.ReformatReads in=mg.assembly.merged.fa out=mg.assembly.merged.length_filtered.fa minlength=10000\n",
    ">Executing jgi.ReformatReads [in=mg.assembly.merged.fa, out=mg.assembly.merged.length_filtered.fa, minlength=10000]\n",
    ">Input is being processed as unpaired\n",
    ">Input:                  \t2184108 reads          \t1337527992 bases\n",
    ">Short Read Discards:    \t2182763 reads (99.94%) \t1310291179 bases (97.96%)\n",
    ">Output:                 \t1345 reads (0.06%) \t27236813 bases (2.04%)\n",
    ">Time:                         \t9.524 seconds.\n",
    ">Reads Processed:       2184k \t229.33k reads/sec\n",
    ">Bases Processed:       1337m \t140.44m bases/sec\n",
    "\n",
    "__Sept 22__\n",
    "\n",
    "- Meeting with Bora 14:00 \n",
    "  - filter preprocessing parameters\n",
    "  - mapping rate \n",
    "\n",
    "\n",
    "__Sept 23__\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 4 Sept 26 - Sept 30 \n",
    "\n",
    "__Sept 26__\n",
    "\n",
    "\n",
    "__Sept 27__\n",
    "\n",
    "- 1 pm meeting with Anna \n",
    "\n",
    "__Sept 28__\n",
    "\n",
    "\n",
    "__Sept 29__\n",
    "\n",
    "\n",
    "__Sept 30__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
